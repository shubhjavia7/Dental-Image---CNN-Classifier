{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1744674822896,
     "user": {
      "displayName": "Shubh Javia",
      "userId": "11939600084809563477"
     },
     "user_tz": 240
    },
    "id": "DiKu4fJ7TaBF",
    "outputId": "ade8b417-aa5b-4e25-bee0-beddbdb703f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1744674823568,
     "user": {
      "displayName": "Shubh Javia",
      "userId": "11939600084809563477"
     },
     "user_tz": 240
    },
    "id": "4ubCD-stUKC9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "\n",
    "    class Block(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, stride):\n",
    "            super().__init__()\n",
    "            kernel_size = 3\n",
    "            padding = (kernel_size - 1) // 2\n",
    "\n",
    "            self.block_model = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size, 1, padding, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "\n",
    "            # Residual connection\n",
    "            if in_channels != out_channels or stride != 1:\n",
    "                self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            else:\n",
    "                self.skip = nn.Identity()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.block_model(x) + self.skip(x)\n",
    "\n",
    "    def __init__(self, channels_l0=64, n_blocks=2, num_classes=8):\n",
    "        super().__init__()\n",
    "        cnn_layers = [\n",
    "            nn.Conv2d(in_channels=3, out_channels=channels_l0, kernel_size=11, stride=2, padding=(11 - 1) // 2, bias=False),\n",
    "            nn.BatchNorm2d(channels_l0),\n",
    "            nn.ReLU(),\n",
    "        ]\n",
    "\n",
    "        c1 = channels_l0\n",
    "        for _ in range(n_blocks):\n",
    "            c2 = c1 * 2\n",
    "            cnn_layers.append(self.Block(c1, c2, stride=2))\n",
    "            c1 = c2\n",
    "\n",
    "        cnn_layers.append(nn.AdaptiveAvgPool2d((1, 1)))  # Global Average Pooling\n",
    "        self.cnn = nn.Sequential(*cnn_layers)\n",
    "        self.classifier = nn.Linear(c1, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten for the classifier\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch.cuda.amp\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for torch.cuda.amp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch.cuda.amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1744674843810,
     "user": {
      "displayName": "Shubh Javia",
      "userId": "11939600084809563477"
     },
     "user_tz": 240
    },
    "id": "sOT86YMiUMkm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "#from torch.amp import autocast, GradScaler\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "data_dir = \"Teeth or Dental Image Dataset\"  # update this to your local path\n",
    "\n",
    "# # Transforms\n",
    "transform = transforms.Compose([\n",
    "     transforms.Resize((128, 128)),   # Resize to make inputs consistent\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    " ])\n",
    "\n",
    "# Apply your full transform pipeline\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Choose how many images you want\n",
    "subset_size = 4000\n",
    "\n",
    "# Split into a subset and discard the rest\n",
    "subset, _ = random_split(dataset, [subset_size, len(dataset) - subset_size])\n",
    "\n",
    "# Now split the subset into train/val\n",
    "train_size = int(0.8 * subset_size)\n",
    "val_size = subset_size - train_size\n",
    "train_subset, val_subset = random_split(subset, [train_size, val_size])\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = ConvNet(channels_l0=32, n_blocks=2, num_classes=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 848629,
     "status": "ok",
     "timestamp": 1744676676822,
     "user": {
      "displayName": "Shubh Javia",
      "userId": "11939600084809563477"
     },
     "user_tz": 240
    },
    "id": "jtVDoWq0pFyn",
    "outputId": "7934c870-79e0-40ff-ddd9-38603da1a5b7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15] Loss: 1.1251 Train Acc: 59.94% Val Acc: 65.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15: 100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15] Loss: 0.5362 Train Acc: 85.50% Val Acc: 74.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15: 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15] Loss: 0.2739 Train Acc: 93.47% Val Acc: 62.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15: 100%|██████████| 100/100 [00:34<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15] Loss: 0.1864 Train Acc: 95.34% Val Acc: 96.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15: 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15] Loss: 0.1454 Train Acc: 96.28% Val Acc: 94.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15: 100%|██████████| 100/100 [00:34<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15] Loss: 0.0964 Train Acc: 97.66% Val Acc: 96.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15: 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15] Loss: 0.0874 Train Acc: 97.75% Val Acc: 95.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15: 100%|██████████| 100/100 [00:34<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15] Loss: 0.0912 Train Acc: 97.47% Val Acc: 98.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15: 100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15] Loss: 0.0562 Train Acc: 98.53% Val Acc: 97.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15: 100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15] Loss: 0.0518 Train Acc: 98.72% Val Acc: 97.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15: 100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15] Loss: 0.0436 Train Acc: 98.91% Val Acc: 96.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15: 100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15] Loss: 0.0497 Train Acc: 98.72% Val Acc: 97.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15: 100%|██████████| 100/100 [00:34<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15] Loss: 0.0463 Train Acc: 98.53% Val Acc: 97.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15: 100%|██████████| 100/100 [00:34<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15] Loss: 0.0439 Train Acc: 98.62% Val Acc: 98.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15: 100%|██████████| 100/100 [00:34<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15] Loss: 0.0292 Train Acc: 99.19% Val Acc: 98.38%\n",
      "Model saved as teeth_cnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Model, loss, optimizer\n",
    "model = ConvNet(channels_l0=32, n_blocks=2, num_classes=8).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "# Initialize GradScaler\n",
    "scaler = GradScaler(device='cuda')\n",
    "\n",
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_preds = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Mixed precision forward pass\n",
    "        with autocast(device_type='cuda'):  # Automatically use mixed precision\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        # Scale the loss and backpropagate\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Step the optimizer and update the scaler\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Track running loss and accuracy\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct_preds += predicted.eq(labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_accuracy = 100. * correct_preds / total\n",
    "    avg_loss = running_loss / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            with autocast(device_type='cuda'):  # Ensure mixed precision during validation as well\n",
    "                outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            val_correct += predicted.eq(labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    val_accuracy = 100. * val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Loss: {avg_loss:.4f} Train Acc: {train_accuracy:.2f}% Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/MSAI/AI in Health/HRP/teeth_cnn_model.pth\")\n",
    "print(\"Model saved as teeth_cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ReJ8F1t-2b6a"
   },
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1744677402158,
     "user": {
      "displayName": "Shubh Javia",
      "userId": "11939600084809563477"
     },
     "user_tz": 240
    },
    "id": "jw_Js6OY3nAK",
    "outputId": "a1dc6f08-e975-4036-d233-bd7a140e26cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Lower Front, Index: 0\n",
      "Class: Lower Left, Index: 1\n",
      "Class: Lower Occlusal, Index: 2\n",
      "Class: Lower Right, Index: 3\n",
      "Class: Upper Front, Index: 4\n",
      "Class: Upper Left, Index: 5\n",
      "Class: Upper Occlusal, Index: 6\n",
      "Class: Upper Right, Index: 7\n"
     ]
    }
   ],
   "source": [
    "class_to_idx = dataset.class_to_idx\n",
    "class_names = []\n",
    "for class_name, idx in class_to_idx.items():\n",
    "    class_names.append(class_name)\n",
    "    print(f\"Class: {class_name}, Index: {idx}\")\n",
    "\n",
    "#print(\"Predicted class label:\", class_names[predicted_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1744677478412,
     "user": {
      "displayName": "Shubh Javia",
      "userId": "11939600084809563477"
     },
     "user_tz": 240
    },
    "id": "JLdKxCvp11vd",
    "outputId": "f964f47e-3dd1-4aa2-93ef-a99ede7eb444"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class index: 3\n",
      "predicted Class Label: Lower Right\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "image_path = \"/Users/shubhjavia/Desktop/MSAI/SP'25/AI in Health/High Risk Tutorial/LR 1238.jpg\"  # Change this\n",
    "image = Image.open(image_path).convert(\"RGB\")  # Ensure it's 3-channel\n",
    "input_tensor = transform(image).unsqueeze(0)  # Add batch dimension: shape [1, 3, H, W]\n",
    "#input_tensor = input_tensor.to(device)  # e.g., device = torch.device(\"cuda\")\n",
    "model.load_state_dict(torch.load(\"/Users/shubhjavia/Desktop/MSAI/SP'25/AI in Health/High Risk Tutorial/teeth_cnn_model.pth\",map_location=torch.device('cpu')))\n",
    "model.eval()  # set to eval mode for inference\n",
    "output = model(input_tensor)\n",
    "predicted_class = output.argmax(1).item()  # Get predicted class index\n",
    "\n",
    "print(\"Predicted class index:\", predicted_class)\n",
    "print(\"predicted Class Label:\",class_names[predicted_class])\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPSKcorfCVOCVYWDRJD0VdS",
   "gpuType": "A100",
   "mount_file_id": "1CWlwKaVp4t28lYC9aLO3TF3QbLgXY_6R",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
